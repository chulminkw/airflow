from airflow.sdk import DAG, task, task_group
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
import pandas as pd
from pendulum import datetime

BUCKET_NAME = "mybucket"
PREFIX = ""

with DAG(
    dag_id="s3_to_postres_with_groups",
    start_date=datetime(2025, 12, 1),
    schedule=None,
    catchup=False,
) as dag:
    # 1. extract group
    @task_group
    def extract_group():

        @task
        def list_s3_files():
            s3 = S3Hook(aws_conn_id="minio_conn")
            files = s3.list_keys(bucket_name=BUCKET_NAME, prefix=PREFIX)
            return [f for f in files if f.endswith(".csv")]
        
        @task
        def extract_s3(file_key):
            """Download s3 file to local airflow worker directory"""
            s3 = S3Hook(aws_conn_id="minio_conn")
            filename = file_key.split("/")[-1]
            local_path = f"/usr/local/airflow/{filename}" # local file storage

            s3.download_file(
                bucket_name=BUCKET_NAME,
                key=file_key,
                local_path=local_path,
                preserve_file_name=False,
                use_autogenerated_subdir=False#True이면 random한 서브디렉토리명을 생성
            )

            return {
                "file_key": file_key,
                "local_path": local_path
            }
        
        # task group내에서 task 실행 및 dependency 설정.
        files = list_s3_files()
        #task 동적 매핑
        extracted_paths = extract_s3.expand(file_key=files)
        return extracted_paths

    @task_group()
    def transform_group(extracted_results):
        @task
        def transform(extracted_result):
            """Transform file loaded from local disk"""
            df = pd.read_csv(extracted_result["local_path"])
            df = df[df["amount"] > 0]

            #write transformed file back to disk
            out_path = extracted_result["local_path"].replace(".csv", "_transformed.csv")
            df.to_csv(out_path, index=False)

            return {"file_key": extracted_result["file_key"], "local_path": out_path}
        
        transformed_paths = transform.expand(extracted_result=extracted_results)
        return transformed_paths
    
    @task_group
    def load_group(records):

        @task
        def load_to_postgres(record: dict):
            pg = PostgresHook(postgres_conn_id="pg_conn")
            conn = pg.get_conn()
            cursor = conn.cursor()

            table_name = "staging_orders"

            with open(record["local_path"], "r") as f:
                cursor.copy_expert(
                    f"COPY {table_name} FROM STDIN WITH CSV HEADER",
                )
            conn.commit()

        load_to_postgres.expand(record=records)

    # dag 내에서 task group 실행...
    extract_task_group = extract_group()
    transform_task_group = transform_group(extract_task_group)
    load_task_group = load_group(transform_task_group)
