from airflow.sdk import DAG, task
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from pendulum import datetime
import os


# s3 Hook의 delete_objects 수행 오류. DAG 파일 상단(import 직후)에 추가
# 최근 boto3 업데이트에서 멀티 객체 삭제(DeleteObjects) 시 Content-MD5 헤더를 자동으로 생성하던 방식이 변경되거나 생략되는 경우가 생겼습니다. 
# AWS S3 서비스는 이 헤더가 필수가 아니게 되었으나, MinIO 구버전 등은 여전히 무결성 검증을 위해 이 헤더를 필수적으로 요구하기 때문에 "Missing required header" 에러가 발생합니다
os.environ["AWS_REQUEST_CHECKSUM_CALCULATION"] = "when_required"
os.environ["AWS_RESPONSE_CHECKSUM_VALIDATION"] = "when_required"

BUCKET_NAME = "mybucket"
AWS_CONN_ID = "minio_conn"
KEY_PREFIX = "sample/"
LOCAL_DIR = "/usr/local/airflow/include/user_data"

with DAG(
    dag_id="sample_s3_interface_02",
    start_date=datetime(2025, 12, 1),
    schedule=None,
    catchup=False
) as dag:
    
    @task
    def list_s3_keys():
        s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        keys = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=KEY_PREFIX)
        return keys
    
    list_s3_keys_task = list_s3_keys()

    @task
    def download_from_s3(keys):
        s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        local_path_list = []
        for key in keys:
            #local_dir
            local_dir = os.path.join(LOCAL_DIR, "from_s3")
            os.makedirs(os.path.dirname(local_dir), exist_ok=True)
            local_path = os.path.join(local_dir, os.path.basename(key))
            print(f"#### key {key}, local_path: {local_path}")
            # 핵심: 파일이 이미 존재하면 먼저 삭제하여 에러 방지
            if os.path.exists(local_path):
                print(f"이미 파일이 존재하여 삭제합니다: {local_path}")
                os.remove(local_path)
            # download_file()의 인자로 preserve_file_name이 True이면 use_autogenerated_subdir=True일 시
            # local file명을 넣으면, 임시 서브디렉토리가 만들어짐. 
            # local_path에 local directory를 넣으면, preserve_file_name=True, use_autogenerated_subdir=True
            # 여도 임시 서브디렉토리가 만들어지지 않음. 
            s3_hook.download_file(key=key, bucket_name=BUCKET_NAME, local_path=local_dir,
                                 preserve_file_name=True, use_autogenerated_subdir=False)
            # local_path에 local_path를 입력하면, preserve_file_name=True use_autogenerated_subdir=False
            # s3_hook.download_file(key=key, bucket_name=BUCKET_NAME, local_path=local_path,
            #                      preserve_file_name=True, use_autogenerated_subdir=False)

            local_path_list.append(local_path)

        return keys
    
    @task
    def delete_s3_object(keys):
        print(f"##### keys: {keys} ###### ")
        s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        s3_hook.delete_objects(bucket=BUCKET_NAME, keys=keys )
        
        return keys
    
    download_from_s3_task = download_from_s3(list_s3_keys_task)
    delete_s3_object(download_from_s3_task)

